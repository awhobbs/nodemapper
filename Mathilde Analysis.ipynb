{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to open saved data.\n",
      "Loading a subset with  100  nodes.\n",
      "Fitting first model (time only)\n",
      "[  31.69180508   44.94237976   66.45392761 ...,   65.72812653   91.78492126\n",
      "  120.4089798 ]\n",
      "Fitting second model (other features)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hobbs/.conda/envs/ugh/lib/python3.6/site-packages/ipykernel/__main__.py:181: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/hobbs/.conda/envs/ugh/lib/python3.6/site-packages/ipykernel/__main__.py:192: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "14\n",
      "[RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=4,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False), RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=4,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hobbs/.conda/envs/ugh/lib/python3.6/site-packages/ipykernel/__main__.py:203: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE time delay only:\n",
      "5.07961586606\n",
      "RMSE time delay + features\n",
      "5.06138775262\n",
      "                                  dollar_mw       y_ts     y_full\n",
      "node         time                                                \n",
      "ALAMT4G_7_B1 2017-10-02 00:00:00  35.283878  30.184175  28.319812\n",
      "             2017-10-02 01:00:00  49.763371  33.618034  34.662872\n",
      "             2017-10-02 02:00:00  58.605629  49.171923  50.823064\n",
      "             2017-10-02 03:00:00  44.077530  62.647497  62.696796\n",
      "             2017-10-02 04:00:00  36.175529  50.247875  50.434281\n",
      "WMAE time delay only:\n",
      "                                      error  dollar_mw  abs_error  week\n",
      "node         time                                                      \n",
      "ALAMT4G_7_B1 2017-10-02 00:00:00  -5.099703  35.283878   5.099703    40\n",
      "             2017-10-02 01:00:00 -16.145336  49.763371  16.145336    40\n",
      "             2017-10-02 02:00:00  -9.433706  58.605629   9.433706    40\n",
      "             2017-10-02 03:00:00  18.569967  44.077530  18.569967    40\n",
      "             2017-10-02 04:00:00  14.072345  36.175529  14.072345    40\n",
      "0.170247913474\n",
      "WMAE time delay + features\n",
      "                                      error  dollar_mw  abs_error  week\n",
      "node         time                                                      \n",
      "ALAMT4G_7_B1 2017-10-02 00:00:00  -6.964067  35.283878   6.964067    40\n",
      "             2017-10-02 01:00:00 -15.100498  49.763371  15.100498    40\n",
      "             2017-10-02 02:00:00  -7.782565  58.605629   7.782565    40\n",
      "             2017-10-02 03:00:00  18.619266  44.077530  18.619266    40\n",
      "             2017-10-02 04:00:00  14.258752  36.175529  14.258752    40\n",
      "0.166905579661\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f28439687f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import itertools\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import patsy\n",
    "from sklearn import preprocessing, linear_model, model_selection, ensemble, metrics\n",
    "from ggplot import *\n",
    "from tqdm import tnrange, tqdm\n",
    "\n",
    "def load_data(quick=True):\n",
    "\n",
    "    def make_pickle():\n",
    "        # Read csv\n",
    "        df = pd.read_csv('nodes_final_data.csv')\n",
    "\n",
    "        # Set the index\n",
    "        df['time'] = pd.to_datetime(df['time'])\n",
    "        df = df.set_index(['node', 'time'])\n",
    "\n",
    "        # Create/ format some variables\n",
    "        df['week'] = [value[1].isocalendar()[1] for value in df.index.values]\n",
    "\n",
    "        # Save as a pickle\n",
    "        df.to_pickle('nodes_final_data.p')\n",
    "\n",
    "        return df\n",
    "\n",
    "    if quick is True:\n",
    "        try:\n",
    "            print(\"Trying to open saved data.\")\n",
    "            with open('nodes_final_data.p', 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(\"No existing pickle found... picklemaking!\")\n",
    "            return make_pickle()\n",
    "    else:\n",
    "        print(\"Pickling a fresh new pickle.\")\n",
    "        return make_pickle()\n",
    "\n",
    "\n",
    "def load_subset(n, df):\n",
    "    print(\"Loading a subset with \", n, \" nodes.\")\n",
    "    np.random.seed(seed=1)\n",
    "    node_ids = df.index.get_level_values('node').unique()\n",
    "    selected_nodes = list(np.random.choice(node_ids, size=n))\n",
    "    return df.loc[selected_nodes]\n",
    "\n",
    "\n",
    "def quick_patsy(arg, input_data, quick=True):\n",
    "    #File will save with the patsy description and number of observations\n",
    "    filename = arg + str(input_data.shape[1]) + '.p'\n",
    "    if quick:\n",
    "        try:\n",
    "            with open(filename, 'rb') as f:\n",
    "                y, X = pickle.load(f)\n",
    "        except FileNotFoundError:\n",
    "            with open(filename, 'wb') as f:\n",
    "                y, X = tuple(\n",
    "                    np.array(matrix)\n",
    "                    for matrix in patsy.dmatrices(arg, data=input_data))\n",
    "                pickle.dump((y, X), f)\n",
    "    else:\n",
    "        with open(filename, 'wb') as f:\n",
    "            y, X = tuple(\n",
    "                np.array(matrix)\n",
    "                for matrix in patsy.dmatrices(arg, data=input_data))\n",
    "            pickle.dump((y, X), f)\n",
    "\n",
    "    y = np.array(y)\n",
    "    X = np.array(X)\n",
    "    return (y, X)\n",
    "\n",
    "\n",
    "normalize = lambda x: (x - np.mean(x))/ np.std(x)\n",
    "\n",
    "def lag_var(df, var, n_periods):\n",
    "    return df[var].groupby(level='node').shift(n_periods)\n",
    "\n",
    "\n",
    "\n",
    "df = load_subset(100, load_data())\n",
    "#df.head()\n",
    "\n",
    "df['temp_last_hr'] = lag_var(df, 'temp', 1)\n",
    "df['price_last_hr'] = lag_var(df, 'dollar_mw', 1)\n",
    "df['price_yesterday'] = lag_var(df, 'dollar_mw', 24)\n",
    "df['price_last_week'] = lag_var(df, 'dollar_mw', 24 * 7)\n",
    "df['nodenorm_temp'] = df['temp'].groupby(level = 'node').apply(normalize)\n",
    "df['node'] = [value[0] for value in df.index.values]\n",
    "\n",
    "lagnames = ''\n",
    "\n",
    "for i in list(range(1, 24 * 7)):\n",
    "    name = 'lag' + str(i)\n",
    "    lagnames += name + ' + '\n",
    "    df[name] = lag_var(df, 'dollar_mw', i)\n",
    "\n",
    "# temperature bins\n",
    "#bins = [np.min(df['nodenorm_temp']), -2, -1, 1, 2, np.max(df['nodenorm_temp'])]\n",
    "#group_names = ['Very Low', 'Low', 'Normal', 'High', 'Very High']\n",
    "#df['temp_bin'] = pd.cut(df['nodenorm_temp'], bins, labels=group_names)\n",
    "\n",
    "# drop NA values, since beginning and ends now lack variables\n",
    "df = df.dropna()\n",
    "\n",
    "# normalize features\n",
    "#to_normalize = ['other_MW', 'solar_MW', 'wind_MW', 'latitude', 'longitude', 'temp']\n",
    "#df[to_normalize] = df[to_normalize].apply(normalize)\n",
    "\n",
    "df.head()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "def rmse(y_actual, y_predicted):\n",
    "    rms = sqrt(mean_squared_error(y_actual, y_predicted))\n",
    "    return np.sqrt(rms)\n",
    "\n",
    "def evaluate(train_index, test_index, model, X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    fitted = model.fit(X_train, y_train)\n",
    "    y_hat = fitted.predict(X_test)\n",
    "    errors = np.subtract(y_test, y_hat)\n",
    "    y_hat_all = fitted.predict(X)\n",
    "    return [rmse(y_test, y_hat),  y_hat_all, errors]\n",
    "\n",
    "def run_models(models, feature_sets, df, folds=8, parallel=True):\n",
    "    '''\n",
    "    Takes a list of models and features, and runs each model with each set of features\n",
    "    Features should be patsy-formatted strings.\n",
    "\n",
    "    Models should be a list of sci-kit learn models and the second\n",
    "    the number of jobs that used for cross-validation.\n",
    "\n",
    "    Data is 'df' (a pandas dataframe), and 'folds' is the number of folds that should be used\n",
    "    for cross-validation.\n",
    "    '''\n",
    "    #iterate through all the models\n",
    "    results = []\n",
    "    kf = model_selection.KFold(n_splits=folds)\n",
    "    for features in tqdm(feature_sets, desc = 'Feature Set'):\n",
    "        y, X = patsy.dmatrices(features, data=df)\n",
    "        y = np.array(y)\n",
    "        X = preprocessing.scale(X)\n",
    "        for model in tqdm(models, desc = \"Models\"):\n",
    "            if parallel:\n",
    "                result = Parallel(n_jobs=folds)(delayed(evaluate)(train_index, test_index, model, X, np.ravel(y)) for train_index, test_index in kf.split(X))\n",
    "            else:\n",
    "                result = [evaluate(train_index, test_index, model, X, np.ravel(y)) for train_index, test_index in kf.split(X)]\n",
    "            scores = [res[0] for res in result]\n",
    "            y_hat = [res[1] for res in result]\n",
    "            errors = [res[2] for res in result]\n",
    "            results.append({'model': model,\n",
    "                            'features': features,\n",
    "                            'score': np.mean(scores),\n",
    "                            'y_hats': y_hat,\n",
    "                            'errors': errors})\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def train_new_model(models, feature_set1, feature_set2, df):\n",
    "\n",
    "    #iterate through all the models\n",
    "    print('Fitting first model (time only)')\n",
    "    results = []\n",
    "    y, X = patsy.dmatrices(feature_set1, data=df)\n",
    "    #X = preprocessing.scale(X)\n",
    "    y = np.array(y)\n",
    "    model = models[0]\n",
    "    fitted = model.fit(X, np.ravel(y))\n",
    "    y_hat = fitted.predict(X)\n",
    "    y_residu = np.ravel(y) - np.ravel(y_hat)\n",
    "\n",
    "    results.append({'model': fitted, 'y_hats': y_hat, 'y_residu': y_residu})\n",
    "    print(results[0]['y_hats'])\n",
    "    print('Fitting second model (other features)')\n",
    "    df_results_1 = pd.DataFrame(results)\n",
    "    df['y_residu'] = df_results_1['y_residu'][0].ravel()\n",
    "    #print(df)\n",
    "\n",
    "\n",
    "    #print(df.columns)\n",
    "    #print(df.keys())\n",
    "    y, X = patsy.dmatrices(feature_set2, data=df)\n",
    "    y = np.array(y)\n",
    "    #X = preprocessing.scale(X)\n",
    "\n",
    "    model = models[1]\n",
    "    fitted = model.fit(X, y)\n",
    "    results.append({'model': fitted})\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def test_new_model(fitted_models, feature_set1, feature_set2, df):\n",
    "    y, X = patsy.dmatrices(feature_set1, data=df, return_type = 'dataframe')\n",
    "    index_values = y.index\n",
    "    #y = np.array(y)\n",
    "    y_hat = fitted_models[0].predict(X)\n",
    "    df['y_residu'] = np.ravel(y) - np.ravel(y_hat)\n",
    "    y2, X2 = patsy.dmatrices(feature_set2, data=df, return_type = 'dataframe')\n",
    "    y_final = fitted_models[1].predict(X2)\n",
    "    res = np.ravel(y) - np.ravel(y_final)\n",
    "    #plt.plot(y_final + y_hat, label='model sum')\n",
    "    #plt.plot(y_hat, label='estimate model 1')\n",
    "    #plt.plot(y, label='original')\n",
    "    #plt.plot(y_final, label='estimate model 2')\n",
    "    #plt.plot(y.ravel() - y_hat.ravel(), label='residuals 1')\n",
    "    #plt.grid()\n",
    "    \n",
    "    \n",
    "    # Calculate y_hats and map to indices\n",
    "    \n",
    "    y_hat = pd.DataFrame(data = fitted_models[0].predict(X), index = index_values)\n",
    "    y_final = pd.DataFrame(data = fitted_models[1].predict(X2), index = index_values)\n",
    "    \n",
    "    plt.legend()\n",
    "    print('RMSE time delay only:')\n",
    "    print(rmse(y, y_hat))\n",
    "    print('RMSE time delay + features')\n",
    "    print(rmse(y, y_final + y_hat))\n",
    "    \n",
    "    def wmae(little_df, df):\n",
    "        # name is wrong as an artifact of how the thing was produced\n",
    "        little_df = little_df.rename(columns={'dollar_mw': 'error'})\n",
    "        # merge with prices\n",
    "        little_df = pd.merge(little_df, df, left_index=True, right_index=True)\n",
    "\n",
    "        # Get absolute value of the error\n",
    "        little_df['abs_error'] = np.absolute(little_df['error'])\n",
    "        # Get week index for grouping\n",
    "        little_df['week']  = [date.isocalendar()[1] for date in little_df.index.get_level_values('time')]\n",
    "        print(little_df.head())\n",
    "        little_df = little_df.rename(columns={'0': 'dollar_mw'})\n",
    "        return np.mean(little_df.groupby('week').mean()['abs_error']/little_df.groupby('week').mean()['dollar_mw'])\n",
    "    \n",
    "    \n",
    "    error_df = y\n",
    "    error_df['y_ts'] = y_hat.values\n",
    "    error_df['y_full'] = y_hat.values + y_final.values\n",
    "    \n",
    "    print(error_df.head())\n",
    "    \n",
    "\n",
    "    print('WMAE time delay only:')\n",
    "    error_df['error'] = error_df['y_ts'] - error_df['dollar_mw']\n",
    "    \n",
    "    print(wmae(pd.DataFrame(error_df['error']), pd.DataFrame(error_df['dollar_mw'])))\n",
    "    print('WMAE time delay + features')\n",
    "    error_df['error'] = error_df['y_full'] - error_df['dollar_mw']\n",
    "    print(wmae(pd.DataFrame(error_df['error']), pd.DataFrame(error_df['dollar_mw'])))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "df_test = df[df['month'] >= 10]\n",
    "df_train = df[df['month'] < 10]\n",
    "\n",
    "# We first chose the model :\n",
    "feature_sets1 = 'dollar_mw ~ price_last_hr + price_yesterday + price_last_week'\n",
    "feature_sets2 = 'y_residu ~ other_MW + solar_MW + wind_MW + load_MW + fuel_price + opr_hr + day + temp + irrad + wind_u + wind_v + latitude + longitude'\n",
    "\n",
    "model1 = ensemble.RandomForestRegressor(n_jobs = 4)\n",
    "model2 = ensemble.RandomForestRegressor(n_jobs = 4)\n",
    "results = train_new_model([model1, model2], feature_sets1, feature_sets2, df_train)\n",
    "fitted_models = [v['model'] for v in results]\n",
    "print(fitted_models[0].n_features_)\n",
    "print(fitted_models[1].n_features_)\n",
    "print(fitted_models)\n",
    "test_new_model(fitted_models, feature_sets1, feature_sets2, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ugh]",
   "language": "python",
   "name": "conda-env-ugh-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

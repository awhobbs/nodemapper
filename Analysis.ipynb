{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hobbs/.local/lib/python3.6/site-packages/ggplot/utils.py:81: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.\n",
      "You can access Timestamp as pandas.Timestamp\n",
      "  pd.tslib.Timestamp,\n",
      "/home/hobbs/.local/lib/python3.6/site-packages/ggplot/stats/smoothers.py:4: FutureWarning: The pandas.lib module is deprecated and will be removed in a future version. These are private functions and can be accessed from pandas._libs.lib instead\n",
      "  from pandas.lib import Timestamp\n",
      "/home/hobbs/.conda/envs/ugh/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import itertools\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import patsy\n",
    "from sklearn import preprocessing, linear_model, model_selection, ensemble, metrics\n",
    "from ggplot import *\n",
    "from tqdm import tnrange, tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to import our data. I'm going to import it once from the csv, and create a pickle below. However, when I re-run this notebook I want to be able to skip steps, so I'll just load the pickle. That is much faster. The argument `quick=False` will force the program to re-create the pickle (for example if we change the csv data or add new variables to the load_data function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(quick=True):\n",
    "    \n",
    "    def make_pickle():\n",
    "        # Read csv\n",
    "        df = pd.read_csv('new_data/data/nodes_final_data.csv')\n",
    "        \n",
    "        # Set the index\n",
    "        df['time'] = pd.to_datetime(df['time'])\n",
    "        df = df.set_index(['node', 'time'])\n",
    "\n",
    "        # Create/ format some variables\n",
    "        df['week'] = [value[1].isocalendar()[1] for value in df.index.values]\n",
    "\n",
    "        # Save as a pickle\n",
    "        df.to_pickle('nodes_final_data.p')\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    if quick == True:\n",
    "        try:\n",
    "            print(\"Trying to open saved data.\")\n",
    "            with open('nodes_final_data.p', 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        \n",
    "        except FileNotFoundError:\n",
    "            print(\"No existing pickle found... picklemaking!\")\n",
    "            return make_pickle()\n",
    "    else:\n",
    "        print(\"Pickling a fresh new pickle.\")\n",
    "        return make_pickle()\n",
    "\n",
    "def load_subset(n, df):\n",
    "    print(\"Loading a subset with \", n, \" nodes.\")\n",
    "    np.random.seed(seed=1)\n",
    "    node_ids = df.index.get_level_values('node').unique()\n",
    "    selected_nodes = list(np.random.choice(node_ids, size = n))\n",
    "    return df.loc[selected_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_subset(1000, load_data(quick=False))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to expand our data into more features. Patsy is good at this, but like the above, it is time consuming to do it repeatedly. Thus, we'll save the results and only recreate them if something changes. The only downside of this is potential proliferation of pickle-matrices - gotta delete these occassionally!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def quick_patsy(arg, input_data, quick=True):\n",
    "     #File will save with the patsy description and number of observations\n",
    "    filename = arg + str(input_data.shape[1]) + '.p'\n",
    "    if quick:\n",
    "        try:\n",
    "            with open(filename, 'rb') as f:\n",
    "                y, X = pickle.load(f)\n",
    "        except FileNotFoundError:\n",
    "            with open(filename, 'wb') as f:\n",
    "                y, X = tuple(np.array(matrix) for matrix in patsy.dmatrices(arg, data = input_data))\n",
    "                pickle.dump((y, X), f)\n",
    "    else:\n",
    "        with open(filename, 'wb') as f:\n",
    "                y, X = tuple(np.array(matrix) for matrix in patsy.dmatrices(arg, data = input_data))\n",
    "                pickle.dump((y, X), f)\n",
    "                \n",
    "    y = np.array(y)\n",
    "    X = np.array(X)\n",
    "    return (y, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to create time and spatial lags. To do that, we can use `shift()` in combination with `groupby()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = lambda x: (x - np.mean(x))/ np.std(x)\n",
    "\n",
    "def lag_var(df, var, n_periods):\n",
    "    return df[var].groupby(level='node').shift(n_periods)\n",
    "\n",
    "df['temp_last_hr'] = lag_var(df, 'temp', 1)\n",
    "df['price_last_hr'] = lag_var(df, 'dollar_mw', 1)\n",
    "df['price_yesterday'] = lag_var(df, 'dollar_mw', 24)\n",
    "df['price_last_week'] = lag_var(df, 'dollar_mw', 24 * 7)\n",
    "df['nodenorm_temp'] = df['temp'].groupby(level = 'node').apply(normalize)\n",
    "df['node'] = [value[0] for value in df.index.values]\n",
    "print(\"Normalizing price\")\n",
    "\n",
    "lagnames = ''\n",
    "\n",
    "for i in list(range(1, 24 * 7)):\n",
    "    name = 'lag' + str(i)\n",
    "    lagnames += name + ' + '\n",
    "    df[name] = lag_var(df, 'dollar_mw', i)\n",
    "\n",
    "# temperature bins\n",
    "#bins = [np.min(df['nodenorm_temp']), -2, -1, 1, 2, np.max(df['nodenorm_temp'])]\n",
    "#group_names = ['Very Low', 'Low', 'Normal', 'High', 'Very High']\n",
    "#df['temp_bin'] = pd.cut(df['nodenorm_temp'], bins, labels=group_names)\n",
    "\n",
    "# drop NA values, since beginning and ends now lack variables\n",
    "df = df.dropna()\n",
    "\n",
    "# normalize features\n",
    "#to_normalize = ['other_MW', 'solar_MW', 'wind_MW', 'latitude', 'longitude', 'temp']\n",
    "#df[to_normalize] = df[to_nbormalize].apply(normalize)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to hold out a true test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['day']  = [date.isocalendar()[1] for date in df.index.get_level_values('time')]\n",
    "\n",
    "np.random.seed(seed=5)\n",
    "# select 4 random test weeks\n",
    "test_days = list(np.random.choice(df['day'].unique(), size = 10))\n",
    "\n",
    "# create test set\n",
    "df_test = df[df['day'].isin(test_days)]\n",
    "\n",
    "#create training set\n",
    "df = df[~df['day'].isin(test_days)]\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Runner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is designed to run models and record the same results for all of them. Changing the code here will change how all models are run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmse(y, y_hat):\n",
    "    return np.sqrt(np.mean(np.power(np.subtract(y, y_hat), 2)))\n",
    "\n",
    "def wmae(little_df, df):\n",
    "    # name is wrong as an artifact of how the thing was produced\n",
    "    little_df = little_df.rename(columns={'dollar_mw': 'error'})\n",
    "    # merge with prices\n",
    "    little_df = pd.merge(little_df, df, left_index=True, right_index=True)\n",
    "    \n",
    "    # Get absolute value of the error\n",
    "    little_df['abs_error'] = np.absolute(little_df['error'])\n",
    "    # Get week index for grouping\n",
    "    little_df['week']  = [date.isocalendar()[1] for date in little_df.index.get_level_values('time')]\n",
    "    return little_df.groupby('week').mean()['abs_error']/little_df.groupby('week').mean()['dollar_mw']\n",
    "\n",
    "def evaluate(train_index, test_index, model, X, y):\n",
    "    # Split into train and test\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Save indices from y\n",
    "    index_values = y_test.index\n",
    "    \n",
    "    fitted = model.fit(X_train, np.ravel(y_train))\n",
    "    # Calculate y_hats and map to indices\n",
    "    y_hat = pd.DataFrame(data = fitted.predict(X_test), index = index_values)\n",
    "    errors = np.subtract(y_test, y_hat)\n",
    "    y_hat_all = fitted.predict(X)\n",
    "    return [rmse(y_test, y_hat),  y_hat, errors]\n",
    "\n",
    "def run_models(models, feature_sets, df, folds=8, parallel=True):\n",
    "    '''\n",
    "    Takes a list of models and features, and runs each model with each set of features\n",
    "    Features should be patsy-formatted strings. \n",
    "    \n",
    "    Models should be a list of sci-kit learn models and the second\n",
    "    the number of jobs that used for cross-validation.\n",
    "    \n",
    "    Data is 'df' (a pandas dataframe), and 'folds' is the number of folds that should be used\n",
    "    for cross-validation.\n",
    "    '''\n",
    "    #iterate through all the models\n",
    "    results = []\n",
    "    error_list = []\n",
    "    kf = model_selection.KFold(n_splits=folds)\n",
    "    for features in tqdm_notebook(feature_sets, desc = 'Feature Set'):\n",
    "        y, X = patsy.dmatrices(features, data=df, return_type = 'dataframe')\n",
    "        # Normalize X\n",
    "        X = preprocessing.scale(X)\n",
    "        for model in tqdm_notebook(models, desc = \"Models\"):\n",
    "            if parallel:\n",
    "                result = Parallel(n_jobs=folds)(delayed(evaluate)(train_index, test_index, model, X, y) for train_index, test_index in kf.split(X))\n",
    "            else:\n",
    "                result = [evaluate(train_index, test_index, model, X, y) for train_index, test_index in kf.split(X)]\n",
    "            scores = [res[0] for res in result]\n",
    "            errors = [res[2] for res in result]\n",
    "            y_hat =  [res[1] for res in result]\n",
    "            results.append({'model': model, \n",
    "                            'features': features, \n",
    "                            'score': np.mean(scores)})\n",
    "            error_list.append(errors)\n",
    "        \n",
    "    return {'results': pd.DataFrame(results), 'errors': error_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since linear models have to be linear, it makes sense to run them with different (larger) sets of features. For example, adding squared and interaction terms makes more sense. A random forest could achieve this kind of linearity without being given the transformed variables, so there's no need to provide it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols = [linear_model.LinearRegression(fit_intercept=True, n_jobs = 3)]\n",
    "\n",
    "#generate a huge list of different elastic nets\n",
    "elastic_nets = [linear_model.ElasticNet(alpha= a, l1_ratio = l, warm_start = True)\n",
    "                for a, l in list(itertools.product(np.linspace(0.5, 1, num = 3), np.linspace(0,1, num = 3)))]\n",
    "\n",
    "feature_ideas = ['dollar_mw ~ price_last_hr + price_yesterday + price_last_week',\n",
    "                 'dollar_mw ~ C(node) + week + np.power(week,2) + solar_MW + wind_MW + temp + irrad + wind_u + wind_v + np.power(temp,2)',\n",
    "                 'dollar_mw ~ C(node) + price_last_hr + price_yesterday + price_last_week + week + np.power(week,2) + solar_MW + wind_MW + temp + irrad + wind_u + wind_v + np.power(temp,2) + np.power(temp,3) + latitude + longitude']\n",
    "\n",
    "linear_results = run_models(ols + elastic_nets, feature_ideas, df, folds=4, parallel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the weekly mean average error (WMAE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_wmae(result_dict):\n",
    "    wmae_list = [np.mean(wmae(error_list[0], df)) for error_list in result_dict['errors']]\n",
    "\n",
    "    result_dict['results']['wmae'] = wmae_list\n",
    "    \n",
    "    return result_dict\n",
    "\n",
    "linear_results = add_wmae(linear_results)\n",
    "\n",
    "linear_results['results'].to_pickle('linear_results.p')\n",
    "\n",
    "linear_results['results']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_ideas = ['dollar_mw ~ opr_hr + week + solar_MW + wind_MW + temp + + irrad + wind_u + wind_v + latitude + longitude',\n",
    "                 'dollar_mw ~ opr_hr + week + solar_MW + wind_MW + latitude + longitude + price_last_hr + price_yesterday + price_last_week',\n",
    "                 'dollar_mw ~ opr_hr + week + solar_MW + wind_MW + temp + + irrad + wind_u + wind_v + latitude + longitude + price_last_hr + price_yesterday + price_last_week']\n",
    "\n",
    "rf = ensemble.RandomForestRegressor(n_jobs = 4)\n",
    "gb = ensemble.GradientBoostingRegressor(max_depth = 10)\n",
    "models = [rf, gb]\n",
    "\n",
    "nonlinear_results = run_models(models, feature_ideas, df)\n",
    "\n",
    "nonlinear_results = add_wmae(nonlinear_results)\n",
    "\n",
    "nonlinear_results['results'].to_pickle('nonlinear_results.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonlinear_results['results']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "Now we can test our chosen models on the held-out data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols = linear_model.LinearRegression(fit_intercept=True, n_jobs = 3)\n",
    "\n",
    "best_nonlinear_features = 'dollar_mw ~ opr_hr + week + solar_MW + wind_MW + temp + irrad + wind_u + wind_v + latitude + longitude + price_last_hr + price_yesterday + price_last_week'\n",
    "best_linear_features = 'dollar_mw ~ price_last_hr + price_yesterday + price_last_week'\n",
    "\n",
    "# Train on full set\n",
    "def full_train_test(df, features, model):\n",
    "    y, X = patsy.dmatrices(features, data=df, return_type = 'dataframe')\n",
    "    X = preprocessing.scale(X)\n",
    "    fitted = model.fit(X, np.ravel(y))\n",
    "    y_test, X_test = patsy.dmatrices(features, data=df_test, return_type = 'dataframe')\n",
    "    # Save indices from y\n",
    "    index_values = y_test.index\n",
    "    # Calculate y_hats and map to indices\n",
    "    X_test = preprocessing.scale(X_test)\n",
    "    y_hat = pd.DataFrame(data = fitted.predict(X_test), index = index_values)\n",
    "    errors = np.subtract(y_test, y_hat)\n",
    "    return {'model': model, 'rmse': rmse(y_test, y_hat), 'errors': errors}\n",
    "\n",
    "final_linear = full_train_test(df, best_linear_features, ols)\n",
    "final_nonlinear = full_train_test(df, best_nonlinear_features, gb)\n",
    "#final_linear = add_wmae(final_linear)\n",
    "\n",
    "print(\"linear: \", np.mean(wmae(final_linear['errors'], df_test)))\n",
    "print(\"nonlinear: \", np.mean(wmae(final_nonlinear['errors'], df_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graphic charts the relationship between temperatures and prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalize = lambda x: (x - np.mean(x)) / np.std(x)\n",
    "\n",
    "avg_price = pd.DataFrame(df[['dollar_mw', 'temp']].groupby(level = 'time').mean())\n",
    "avg_price['time'] = avg_price.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_price[['temp', 'dollar_mw']] = avg_price[['temp', 'dollar_mw']].apply(normalize)\n",
    "\n",
    "ggplot(avg_price, aes(x = 'time')) + \\\n",
    "    geom_line(aes(y = 'dollar_mw')) + \\\n",
    "    geom_line(aes(y = 'temp', color = 'red'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_ideas = ['dollar_mw ~ price_last_hr + price_yesterday + price_last_week']\n",
    "\n",
    "rf = ensemble.RandomForestRegressor(n_jobs = 4)\n",
    "gb = ensemble.GradientBoostingRegressor(max_depth = 10)\n",
    "models = [rf, gb]\n",
    "\n",
    "nonlinear_results = run_models(models, feature_ideas, df)\n",
    "nonlinear_results.to_pickle('new_nonlinear_results.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ols = [linear_model.LinearRegression(fit_intercept=False, n_jobs = 3)]\n",
    "\n",
    "#generate a huge list of different elastic nets\n",
    "elastic_nets = [linear_model.ElasticNet(alpha= a, l1_ratio = l, warm_start = True)\n",
    "                for a, l in list(itertools.product(np.linspace(0.5, 1, num = 3), np.linspace(0,1, num = 3)))]\n",
    "\n",
    "feature_ideas = ['dollar_mw ~ C(node)*(price_last_hr + price_yesterday + price_last_week)']\n",
    "\n",
    "linear_results = run_models(ols, feature_ideas, df, folds=4, parallel=True)\n",
    "\n",
    "linear_results.to_pickle('new_linear_results.p')\n",
    "\n",
    "linear_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A single\n",
    "linear_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "model = 'dollar_mw ~ C(node)*(price_last_hr + price_yesterday + price_last_week)'\n",
    "\n",
    "model = smf.ols(model, data = df)\n",
    "res = model.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "model = 'dollar_mw ~ price_last_hr + price_yesterday + price_last_week'\n",
    "\n",
    "model = smf.ols(model, data = df.loc['ALAMT4G_7_B1'])\n",
    "res = model.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nonlinear_results.iloc[0]['errors'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nonlinear_results.iloc[0]['errors'][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ugh]",
   "language": "python",
   "name": "conda-env-ugh-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
